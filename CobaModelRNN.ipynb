{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45344c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724937ef",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88467dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library berhasil diimport!\n",
      "Data berhasil dimuat!\n",
      "\n",
      "============================================================\n",
      "SCRIPT SELESAI!\n",
      "============================================================\n",
      "\n",
      "Catatan:\n",
      "- Uncomment bagian-bagian kode sesuai kebutuhan\n",
      "- Pastikan data sudah dimuat dengan benar sebelum menjalankan\n",
      "- Sesuaikan path file untuk load dan save data/model\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Rainfall Forecasting using Recurrent Neural Networks (RNN-LSTM)\n",
    "# Berdasarkan paper: Prasetya & Djamal (2019)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed untuk reproducibility\n",
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "print(\"Library berhasil diimport!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. LOAD DATA\n",
    "# ============================================================================\n",
    "# Ganti dengan path file Anda\n",
    "df = pd.read_excel('updated_dataset_final.xlsx')\n",
    "\n",
    "\n",
    "# UNCOMMENT baris berikut dan sesuaikan path file:\n",
    "df = pd.read_excel('updated_dataset_final.xlsx')\n",
    "df['Tanggal'] = pd.to_datetime(df['Tanggal'])\n",
    "df = df.sort_values('Tanggal').reset_index(drop=True)\n",
    "\n",
    "print(\"Data berhasil dimuat!\")\n",
    "# print(f\"Jumlah data: {len(df)}\")\n",
    "# print(f\"\\nInfo data:\\n{df.info()}\")\n",
    "# print(f\"\\nSample data:\\n{df.head()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PREPROCESSING DATA\n",
    "# ============================================================================\n",
    "\n",
    "def interpolate_missing_data(df):\n",
    "    \"\"\"\n",
    "    Menangani missing data dengan interpolasi linear\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Interpolasi untuk kolom numerik\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df_clean[col] = df_clean[col].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def extract_weekly_features(df):\n",
    "    \"\"\"\n",
    "    Mengekstrak fitur mingguan dengan mengambil nilai maksimum setiap minggu\n",
    "    Sesuai dengan formula di paper: x = Î£(n/k) * max(xi, x_i+1+7)\n",
    "    \"\"\"\n",
    "    df_weekly = df.copy()\n",
    "    df_weekly['Week'] = df_weekly['Tanggal'].dt.isocalendar().week\n",
    "    df_weekly['Year'] = df_weekly['Tanggal'].dt.year\n",
    "    \n",
    "    # Agregasi mingguan - ambil nilai maksimum\n",
    "    weekly_data = df_weekly.groupby(['Year', 'Week']).agg({\n",
    "        'Temperatur Minimum': 'max',\n",
    "        'Temperatur Maksimum': 'max',\n",
    "        'Temperatur Rata-rata': 'max',\n",
    "        'Kelembapan Rata-rata': 'max',\n",
    "        'Curah Hujan (mm)': 'max',\n",
    "        'Lamanya Penyinaran Matahari': 'max',\n",
    "        'Kecepatan Angin Maksimum': 'max',\n",
    "        'Kecepatan Angin Rata-rata': 'max',\n",
    "        'Tanggal': 'last'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return weekly_data\n",
    "\n",
    "def normalize_data(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalisasi data ke rentang [0, 1]\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "    return df_normalized, scaler\n",
    "\n",
    "def classify_rainfall(rainfall):\n",
    "    \"\"\"\n",
    "    Klasifikasi curah hujan menjadi 5 kelas:\n",
    "    0: Very Low (<5mm)\n",
    "    1: Low (5-20mm)\n",
    "    2: Medium (20-50mm)\n",
    "    3: High (50-100mm)\n",
    "    4: Very High (>100mm)\n",
    "    \"\"\"\n",
    "    if rainfall < 5:\n",
    "        return 0\n",
    "    elif rainfall < 20:\n",
    "        return 1\n",
    "    elif rainfall < 50:\n",
    "        return 2\n",
    "    elif rainfall < 100:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Uncomment untuk menjalankan preprocessing:\n",
    "\n",
    "# 3.1 Interpolasi missing data\n",
    "df = interpolate_missing_data(df)\n",
    "print(\"Missing data berhasil diinterpolasi!\")\n",
    "\n",
    "# 3.2 Ekstraksi fitur mingguan\n",
    "df_weekly = extract_weekly_features(df)\n",
    "print(f\"Data mingguan berhasil diekstrak! Total: {len(df_weekly)} minggu\")\n",
    "\n",
    "# 3.3 Klasifikasi curah hujan\n",
    "df_weekly['Rainfall_Class'] = df_weekly['Curah Hujan (mm)'].apply(classify_rainfall)\n",
    "\n",
    "# 3.4 Normalisasi\n",
    "feature_cols = ['Temperatur Minimum', 'Temperatur Maksimum', 'Temperatur Rata-rata',\n",
    "                'Kelembapan Rata-rata', 'Curah Hujan (mm)', 'Lamanya Penyinaran Matahari',\n",
    "                'Kecepatan Angin Maksimum', 'Kecepatan Angin Rata-rata']\n",
    "\n",
    "df_normalized, scaler = normalize_data(df_weekly, feature_cols)\n",
    "print(\"Data berhasil dinormalisasi!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PREPARE SEQUENCES (OVERLAPPING WINDOWS)\n",
    "# ============================================================================\n",
    "\n",
    "def create_sequences(data, feature_cols, target_col, sequence_length=52):\n",
    "    \"\"\"\n",
    "    Membuat sequences dengan overlap untuk training RNN\n",
    "    Setiap sequence berisi data 1 tahun (52 minggu)\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame yang sudah dinormalisasi\n",
    "    - feature_cols: list kolom fitur\n",
    "    - target_col: kolom target (Rainfall_Class)\n",
    "    - sequence_length: panjang sequence (default 52 minggu = 1 tahun)\n",
    "    \n",
    "    Returns:\n",
    "    - X: array of sequences (samples, timesteps, features)\n",
    "    - y: array of targets\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # Ambil sequence 52 minggu\n",
    "        sequence = data[feature_cols].iloc[i:i+sequence_length].values\n",
    "        # Target adalah minggu ke-53 (prediksi minggu depan)\n",
    "        target = data[target_col].iloc[i+sequence_length]\n",
    "        \n",
    "        X.append(sequence)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Uncomment untuk membuat sequences:\n",
    "\"\"\"\n",
    "sequence_length = 52  # 1 tahun = 52 minggu\n",
    "\n",
    "X, y = create_sequences(df_normalized, feature_cols, 'Rainfall_Class', sequence_length)\n",
    "\n",
    "print(f\"Shape X: {X.shape}\")  # (samples, 52, n_features)\n",
    "print(f\"Shape y: {y.shape}\")  # (samples,)\n",
    "print(f\"Total sequences: {len(X)}\")\n",
    "\n",
    "# Visualisasi distribusi kelas\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=5, edgecolor='black')\n",
    "plt.xlabel('Rainfall Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Rainfall Classes')\n",
    "plt.xticks([0, 1, 2, 3, 4], ['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "class_names = ['Very Low\\n(<5mm)', 'Low\\n(5-20mm)', 'Medium\\n(20-50mm)', \n",
    "               'High\\n(50-100mm)', 'Very High\\n(>100mm)']\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "plt.bar(unique, counts, edgecolor='black')\n",
    "plt.xlabel('Rainfall Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rainfall Class Distribution')\n",
    "plt.xticks(unique, class_names, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SPLIT DATA (80% TRAINING, 20% TESTING)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk split data:\n",
    "\"\"\"\n",
    "# Split data sesuai paper: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Convert target ke categorical (one-hot encoding)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=5)\n",
    "y_test_cat = to_categorical(y_test, num_classes=5)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 6. BUILD RNN-LSTM MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def build_lstm_model(input_shape, num_classes=5, lstm_units=64, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Membangun model RNN-LSTM sesuai paper\n",
    "    \n",
    "    Architecture:\n",
    "    - Input Layer: 64 neurons\n",
    "    - Hidden Layer (LSTM): 64 neurons\n",
    "    - Dropout: 0.2\n",
    "    - Dense Layer: 13 neurons\n",
    "    - Output Layer: 5 classes (softmax)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input + LSTM Layer (64 units)\n",
    "        LSTM(lstm_units, input_shape=input_shape, return_sequences=False),\n",
    "        \n",
    "        # Dropout untuk mencegah overfitting\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense Layer (13 neurons dengan ReLU activation)\n",
    "        Dense(13, activation='relu'),\n",
    "        \n",
    "        # Output Layer (5 classes dengan Sigmoid activation)\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment untuk build model:\n",
    "\"\"\"\n",
    "# Input shape: (sequence_length, n_features)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "model = build_lstm_model(input_shape, num_classes=5, lstm_units=64, dropout_rate=0.2)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 7. COMPILE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk compile model:\n",
    "\"\"\"\n",
    "# Sesuai paper: menggunakan Adam optimizer dengan learning rate 0.001\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model berhasil dikompilasi!\")\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 8. TRAINING MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk training:\n",
    "\"\"\"\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_rainfall_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training dengan 500 epochs sesuai paper\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training selesai!\")\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 9. VISUALISASI HASIL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Visualisasi accuracy dan loss selama training\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment untuk visualisasi:\n",
    "\"\"\"\n",
    "plot_training_history(history)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 10. EVALUASI MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk evaluasi:\n",
    "\"\"\"\n",
    "# Evaluasi pada training data\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluasi pada test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Prediksi\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_names = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 11. COMPARISON: RNN vs CNN (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "def build_cnn_model(input_shape, num_classes=5):\n",
    "    \"\"\"\n",
    "    Build 1D CNN model untuk perbandingan\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment untuk perbandingan RNN vs CNN:\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERBANDINGAN RNN vs CNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = build_cnn_model(input_shape, num_classes=5)\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train CNN\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_train_loss, cnn_train_acc = cnn_model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# Comparison Table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['RNN (LSTM)', 'CNN (1D)'],\n",
    "    'Training Accuracy (%)': [train_accuracy*100, cnn_train_acc*100],\n",
    "    'Training Loss': [train_loss, cnn_train_loss],\n",
    "    'Test Accuracy (%)': [test_accuracy*100, cnn_test_acc*100],\n",
    "    'Test Loss': [test_loss, cnn_test_loss]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 12. COMPARISON: SGD vs ADAM OPTIMIZER (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk perbandingan SGD vs Adam:\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERBANDINGAN SGD vs ADAM OPTIMIZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model dengan SGD\n",
    "model_sgd = build_lstm_model(input_shape, num_classes=5)\n",
    "model_sgd.compile(\n",
    "    optimizer=SGD(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train dengan SGD\n",
    "history_sgd = model_sgd.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate SGD\n",
    "sgd_train_loss, sgd_train_acc = model_sgd.evaluate(X_train, y_train_cat, verbose=0)\n",
    "sgd_test_loss, sgd_test_acc = model_sgd.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# Comparison Table\n",
    "optimizer_comparison = pd.DataFrame({\n",
    "    'Optimizer': ['Adam', 'SGD'],\n",
    "    'Training Accuracy (%)': [train_accuracy*100, sgd_train_acc*100],\n",
    "    'Training Loss': [train_loss, sgd_train_loss],\n",
    "    'Test Accuracy (%)': [test_accuracy*100, sgd_test_acc*100],\n",
    "    'Test Loss': [test_loss, sgd_test_loss]\n",
    "})\n",
    "\n",
    "print(optimizer_comparison.to_string(index=False))\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 13. TESTING DIFFERENT LEARNING RATES (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk testing learning rates:\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING DIFFERENT LEARNING RATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "learning_rates = [0.001, 0.002, 0.010, 0.040, 0.100, 0.400, 0.600, 0.800]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    \n",
    "    # Build and compile model\n",
    "    model_lr = build_lstm_model(input_shape, num_classes=5)\n",
    "    model_lr.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history_lr = model_lr.fit(\n",
    "        X_train, y_train_cat,\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test_cat),\n",
    "        callbacks=[EarlyStopping(patience=30, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss_lr, train_acc_lr = model_lr.evaluate(X_train, y_train_cat, verbose=0)\n",
    "    test_loss_lr, test_acc_lr = model_lr.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    \n",
    "    lr_results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Training Accuracy (%)': train_acc_lr*100,\n",
    "        'Training Loss': train_loss_lr,\n",
    "        'Test Accuracy (%)': test_acc_lr*100,\n",
    "        'Test Loss': test_loss_lr\n",
    "    })\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"\\n\" + lr_df.to_string(index=False))\n",
    "\n",
    "# Visualize learning rate comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(lr_df['Learning Rate'], lr_df['Training Accuracy (%)'], \n",
    "             marker='o', label='Training', linewidth=2)\n",
    "axes[0].plot(lr_df['Learning Rate'], lr_df['Test Accuracy (%)'], \n",
    "             marker='s', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Accuracy vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "axes[1].plot(lr_df['Learning Rate'], lr_df['Training Loss'], \n",
    "             marker='o', label='Training', linewidth=2)\n",
    "axes[1].plot(lr_df['Learning Rate'], lr_df['Test Loss'], \n",
    "             marker='s', label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Loss vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 14. PREDICTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def predict_rainfall(model, input_sequence, scaler):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memprediksi curah hujan minggu depan\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained model\n",
    "    - input_sequence: sequence data 52 minggu terakhir (normalized)\n",
    "    - scaler: scaler yang digunakan untuk normalisasi\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_class: kelas prediksi (0-4)\n",
    "    - class_name: nama kelas\n",
    "    - confidence: confidence score\n",
    "    \"\"\"\n",
    "    # Reshape untuk prediksi\n",
    "    input_seq = input_sequence.reshape(1, input_sequence.shape[0], input_sequence.shape[1])\n",
    "    \n",
    "    # Prediksi\n",
    "    prediction_probs = model.predict(input_seq, verbose=0)\n",
    "    predicted_class = np.argmax(prediction_probs, axis=1)[0]\n",
    "    confidence = prediction_probs[0][predicted_class] * 100\n",
    "    \n",
    "    # Mapping kelas ke nama\n",
    "    class_mapping = {\n",
    "        0: 'Very Low (<5mm)',\n",
    "        1: 'Low (5-20mm)',\n",
    "        2: 'Medium (20-50mm)',\n",
    "        3: 'High (50-100mm)',\n",
    "        4: 'Very High (>100mm)'\n",
    "    }\n",
    "    \n",
    "    class_name = class_mapping[predicted_class]\n",
    "    \n",
    "    return predicted_class, class_name, confidence\n",
    "\n",
    "# Uncomment untuk testing prediksi:\n",
    "\"\"\"\n",
    "# Contoh prediksi\n",
    "sample_sequence = X_test[0]\n",
    "pred_class, pred_name, pred_confidence = predict_rainfall(model, sample_sequence, scaler)\n",
    "\n",
    "print(f\"\\nPrediksi Curah Hujan Minggu Depan:\")\n",
    "print(f\"Kelas: {pred_name}\")\n",
    "print(f\"Confidence: {pred_confidence:.2f}%\")\n",
    "print(f\"Actual: {class_mapping[y_test[0]]}\")\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 15. SAVE AND LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk save model:\n",
    "\"\"\"\n",
    "# Save model\n",
    "model.save('rainfall_forecast_model.h5')\n",
    "print(\"Model berhasil disimpan!\")\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Scaler berhasil disimpan!\")\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment untuk load model:\n",
    "\"\"\"\n",
    "# Load model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('rainfall_forecast_model.h5')\n",
    "\n",
    "# Load scaler\n",
    "loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "print(\"Model dan scaler berhasil dimuat!\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCRIPT SELESAI!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCatatan:\")\n",
    "print(\"- Uncomment bagian-bagian kode sesuai kebutuhan\")\n",
    "print(\"- Pastikan data sudah dimuat dengan benar sebelum menjalankan\")\n",
    "print(\"- Sesuaikan path file untuk load dan save data/model\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
