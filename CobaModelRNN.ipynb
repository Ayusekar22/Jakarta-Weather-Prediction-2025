{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45344c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724937ef",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88467dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall Forecasting using Recurrent Neural Networks (RNN-LSTM)\n",
    "# Berdasarkan paper: Prasetya & Djamal (2019)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed untuk reproducibility\n",
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "print(\"Library berhasil diimport!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. LOAD DATA\n",
    "# ============================================================================\n",
    "# Ganti dengan path file Anda\n",
    "df = pd.read_excel('updated_dataset_final - Copy.xlsx')\n",
    "\n",
    "\n",
    "# UNCOMMENT baris berikut dan sesuaikan path file:\n",
    "df['Tanggal'] = pd.to_datetime(df['Tanggal'])\n",
    "df = df.sort_values('Tanggal').reset_index(drop=True)\n",
    "\n",
    "print(\"Data berhasil dimuat!\")\n",
    "# print(f\"Jumlah data: {len(df)}\")\n",
    "# print(f\"\\nInfo data:\\n{df.info()}\")\n",
    "# print(f\"\\nSample data:\\n{df.head()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PREPROCESSING DATA\n",
    "# ============================================================================\n",
    "\n",
    "def interpolate_missing_data(df):\n",
    "    \"\"\"\n",
    "    Menangani missing data dengan interpolasi linear\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Interpolasi untuk kolom numerik\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df_clean[col] = df_clean[col].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def extract_weekly_features(df):\n",
    "    \"\"\"\n",
    "    Mengekstrak fitur mingguan dengan mengambil nilai maksimum setiap minggu\n",
    "    Sesuai dengan formula di paper: x = Î£(n/k) * max(xi, x_i+1+7)\n",
    "    \"\"\"\n",
    "    df_weekly = df.copy()\n",
    "    df_weekly['Week'] = df_weekly['Tanggal'].dt.isocalendar().week\n",
    "    df_weekly['Year'] = df_weekly['Tanggal'].dt.year\n",
    "    \n",
    "    # Agregasi mingguan - ambil nilai maksimum\n",
    "    weekly_data = df_weekly.groupby(['Year', 'Week']).agg({\n",
    "        'Temperatur Minimum': 'max',\n",
    "        'Temperatur Maksimum': 'max',\n",
    "        'Temperatur Rata-rata': 'max',\n",
    "        'Kelembapan Rata-rata': 'max',\n",
    "        'Curah Hujan (mm)': 'max',\n",
    "        'Lamanya Penyinaran Matahari': 'max',\n",
    "        'Kecepatan Angin Maksimum': 'max',\n",
    "        'Kecepatan Angin Rata-rata': 'max',\n",
    "        'Tanggal': 'last'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return weekly_data\n",
    "\n",
    "def normalize_data(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalisasi data ke rentang [0, 1]\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "    return df_normalized, scaler\n",
    "\n",
    "def classify_rainfall(rainfall):\n",
    "    \"\"\"\n",
    "    Klasifikasi curah hujan menjadi 5 kelas:\n",
    "    0: Very Low (<5mm)\n",
    "    1: Low (5-20mm)\n",
    "    2: Medium (20-50mm)\n",
    "    3: High (50-100mm)\n",
    "    4: Very High (>100mm)\n",
    "    \"\"\"\n",
    "    if rainfall < 5:\n",
    "        return 0\n",
    "    elif rainfall < 20:\n",
    "        return 1\n",
    "    elif rainfall < 50:\n",
    "        return 2\n",
    "    elif rainfall < 100:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Uncomment untuk menjalankan preprocessing:\n",
    "\n",
    "# 3.1 Interpolasi missing data\n",
    "df = interpolate_missing_data(df)\n",
    "print(\"Missing data berhasil diinterpolasi!\")\n",
    "\n",
    "# 3.2 Ekstraksi fitur mingguan\n",
    "df_weekly = extract_weekly_features(df)\n",
    "print(f\"Data mingguan berhasil diekstrak! Total: {len(df_weekly)} minggu\")\n",
    "\n",
    "# 3.3 Klasifikasi curah hujan\n",
    "df_weekly['Rainfall_Class'] = df_weekly['Curah Hujan (mm)'].apply(classify_rainfall)\n",
    "\n",
    "# 3.4 Normalisasi\n",
    "feature_cols = ['Temperatur Minimum', 'Temperatur Maksimum', 'Temperatur Rata-rata',\n",
    "                'Kelembapan Rata-rata', 'Curah Hujan (mm)', 'Lamanya Penyinaran Matahari',\n",
    "                'Kecepatan Angin Maksimum', 'Kecepatan Angin Rata-rata']\n",
    "\n",
    "df_normalized, scaler = normalize_data(df_weekly, feature_cols)\n",
    "print(\"Data berhasil dinormalisasi!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PREPARE SEQUENCES (OVERLAPPING WINDOWS)\n",
    "# ============================================================================\n",
    "\n",
    "def create_sequences(data, feature_cols, target_col, sequence_length=52):\n",
    "\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # Ambil sequence 52 minggu\n",
    "        sequence = data[feature_cols].iloc[i:i+sequence_length].values\n",
    "        # Target adalah minggu ke-53 (prediksi minggu depan)\n",
    "        target = data[target_col].iloc[i+sequence_length]\n",
    "        \n",
    "        X.append(sequence)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Uncomment untuk membuat sequences:\n",
    "\n",
    "sequence_length = 52  # 1 tahun = 52 minggu\n",
    "\n",
    "X, y = create_sequences(df_normalized, feature_cols, 'Rainfall_Class', sequence_length)\n",
    "\n",
    "print(f\"Shape X: {X.shape}\")  # (samples, 52, n_features)\n",
    "print(f\"Shape y: {y.shape}\")  # (samples,)\n",
    "print(f\"Total sequences: {len(X)}\")\n",
    "\n",
    "# Visualisasi distribusi kelas\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=5, edgecolor='black')\n",
    "plt.xlabel('Rainfall Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Rainfall Classes')\n",
    "plt.xticks([0, 1, 2, 3, 4], ['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "class_names = ['Very Low\\n(<5mm)', 'Low\\n(5-20mm)', 'Medium\\n(20-50mm)', \n",
    "               'High\\n(50-100mm)', 'Very High\\n(>100mm)']\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "plt.bar(unique, counts, edgecolor='black')\n",
    "plt.xlabel('Rainfall Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rainfall Class Distribution')\n",
    "plt.xticks(unique, class_names, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SPLIT DATA (80% TRAINING, 20% TESTING)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk split data:\n",
    "\n",
    "# Split data sesuai paper: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Convert target ke categorical (one-hot encoding)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=5)\n",
    "y_test_cat = to_categorical(y_test, num_classes=5)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. BUILD RNN-LSTM MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def build_lstm_model(input_shape, num_classes=5, lstm_units=64, dropout_rate=0.2):\n",
    "    model = Sequential([\n",
    "        # Input + LSTM Layer (64 units)\n",
    "        LSTM(lstm_units, input_shape=input_shape, return_sequences=False),\n",
    "        \n",
    "        # Dropout untuk mencegah overfitting\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense Layer (13 neurons dengan ReLU activation)\n",
    "        Dense(13, activation='relu'),\n",
    "        \n",
    "        # Output Layer (5 classes dengan Sigmoid activation)\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment untuk build model:\n",
    "# Input shape: (sequence_length, n_features)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "model = build_lstm_model(input_shape, num_classes=5, lstm_units=64, dropout_rate=0.2)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. COMPILE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk compile model:\n",
    "\n",
    "# Sesuai paper: menggunakan Adam optimizer dengan learning rate 0.001\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model berhasil dikompilasi!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. TRAINING MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk training:\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_rainfall_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training dengan 500 epochs sesuai paper\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training selesai!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. VISUALISASI HASIL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment untuk visualisasi:\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 10. EVALUASI MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk evaluasi:\n",
    "\n",
    "# Evaluasi pada training data\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluasi pada test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Prediksi\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_names = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 11. COMPARISON: RNN vs CNN (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "def build_cnn_model(input_shape, num_classes=5):\n",
    "    from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment untuk perbandingan RNN vs CNN:\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERBANDINGAN RNN vs CNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = build_cnn_model(input_shape, num_classes=5)\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train CNN\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_train_loss, cnn_train_acc = cnn_model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# Comparison Table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['RNN (LSTM)', 'CNN (1D)'],\n",
    "    'Training Accuracy (%)': [train_accuracy*100, cnn_train_acc*100],\n",
    "    'Training Loss': [train_loss, cnn_train_loss],\n",
    "    'Test Accuracy (%)': [test_accuracy*100, cnn_test_acc*100],\n",
    "    'Test Loss': [test_loss, cnn_test_loss]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 12. COMPARISON: SGD vs ADAM OPTIMIZER (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk perbandingan SGD vs Adam:\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERBANDINGAN SGD vs ADAM OPTIMIZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model dengan SGD\n",
    "model_sgd = build_lstm_model(input_shape, num_classes=5)\n",
    "model_sgd.compile(\n",
    "    optimizer=SGD(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train dengan SGD\n",
    "history_sgd = model_sgd.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate SGD\n",
    "sgd_train_loss, sgd_train_acc = model_sgd.evaluate(X_train, y_train_cat, verbose=0)\n",
    "sgd_test_loss, sgd_test_acc = model_sgd.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# Comparison Table\n",
    "optimizer_comparison = pd.DataFrame({\n",
    "    'Optimizer': ['Adam', 'SGD'],\n",
    "    'Training Accuracy (%)': [train_accuracy*100, sgd_train_acc*100],\n",
    "    'Training Loss': [train_loss, sgd_train_loss],\n",
    "    'Test Accuracy (%)': [test_accuracy*100, sgd_test_acc*100],\n",
    "    'Test Loss': [test_loss, sgd_test_loss]\n",
    "})\n",
    "\n",
    "print(optimizer_comparison.to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 13. TESTING DIFFERENT LEARNING RATES (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk testing learning rates:\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING DIFFERENT LEARNING RATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "learning_rates = [0.001, 0.002, 0.010, 0.040, 0.100, 0.400, 0.600, 0.800]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    \n",
    "    # Build and compile model\n",
    "    model_lr = build_lstm_model(input_shape, num_classes=5)\n",
    "    model_lr.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history_lr = model_lr.fit(\n",
    "        X_train, y_train_cat,\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test_cat),\n",
    "        callbacks=[EarlyStopping(patience=30, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss_lr, train_acc_lr = model_lr.evaluate(X_train, y_train_cat, verbose=0)\n",
    "    test_loss_lr, test_acc_lr = model_lr.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    \n",
    "    lr_results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Training Accuracy (%)': train_acc_lr*100,\n",
    "        'Training Loss': train_loss_lr,\n",
    "        'Test Accuracy (%)': test_acc_lr*100,\n",
    "        'Test Loss': test_loss_lr\n",
    "    })\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"\\n\" + lr_df.to_string(index=False))\n",
    "\n",
    "# Visualize learning rate comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(lr_df['Learning Rate'], lr_df['Training Accuracy (%)'], \n",
    "             marker='o', label='Training', linewidth=2)\n",
    "axes[0].plot(lr_df['Learning Rate'], lr_df['Test Accuracy (%)'], \n",
    "             marker='s', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Accuracy vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "axes[1].plot(lr_df['Learning Rate'], lr_df['Training Loss'], \n",
    "             marker='o', label='Training', linewidth=2)\n",
    "axes[1].plot(lr_df['Learning Rate'], lr_df['Test Loss'], \n",
    "             marker='s', label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Loss vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 14. PREDICTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def predict_rainfall(model, input_sequence, scaler):\n",
    "    # Reshape untuk prediksi\n",
    "    input_seq = input_sequence.reshape(1, input_sequence.shape[0], input_sequence.shape[1])\n",
    "    \n",
    "    # Prediksi\n",
    "    prediction_probs = model.predict(input_seq, verbose=0)\n",
    "    predicted_class = np.argmax(prediction_probs, axis=1)[0]\n",
    "    confidence = prediction_probs[0][predicted_class] * 100\n",
    "    \n",
    "    # Mapping kelas ke nama\n",
    "    class_mapping = {\n",
    "        0: 'Very Low (<5mm)',\n",
    "        1: 'Low (5-20mm)',\n",
    "        2: 'Medium (20-50mm)',\n",
    "        3: 'High (50-100mm)',\n",
    "        4: 'Very High (>100mm)'\n",
    "    }\n",
    "    \n",
    "    class_name = class_mapping[predicted_class]\n",
    "    \n",
    "    return predicted_class, class_name, confidence\n",
    "\n",
    "# Uncomment untuk testing prediksi\n",
    "# Contoh prediksi\n",
    "sample_sequence = X_test[0]\n",
    "pred_class, pred_name, pred_confidence = predict_rainfall(model, sample_sequence, scaler)\n",
    "\n",
    "print(f\"\\nPrediksi Curah Hujan Minggu Depan:\")\n",
    "print(f\"Kelas: {pred_name}\")\n",
    "print(f\"Confidence: {pred_confidence:.2f}%\")\n",
    "print(f\"Actual: {class_mapping[y_test[0]]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 15. SAVE AND LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk save model:\n",
    "\n",
    "# Save model\n",
    "model.save('rainfall_forecast_model.h5')\n",
    "print(\"Model berhasil disimpan!\")\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Scaler berhasil disimpan!\")\n",
    "\n",
    "\n",
    "# Uncomment untuk load model:\n",
    "\n",
    "# Load model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('rainfall_forecast_model.h5')\n",
    "\n",
    "# Load scaler\n",
    "loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "print(\"Model dan scaler berhasil dimuat!\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCRIPT SELESAI!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCatatan:\")\n",
    "print(\"- Uncomment bagian-bagian kode sesuai kebutuhan\")\n",
    "print(\"- Pastikan data sudah dimuat dengan benar sebelum menjalankan\")\n",
    "print(\"- Sesuaikan path file untuk load dan save data/model\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af428ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall Forecasting using Recurrent Neural Networks (RNN-LSTM)\n",
    "# Berdasarkan paper: Prasetya & Djamal (2019)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed untuk reproducibility\n",
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "print(\"Library berhasil diimport!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. LOAD DATA\n",
    "# ============================================================================\n",
    "# Ganti dengan path file Anda\n",
    "df = pd.read_excel('updated_dataset_final - Copy.xlsx')\n",
    "\n",
    "# Contoh struktur data yang diharapkan:\n",
    "\n",
    "\n",
    "# UNCOMMENT baris berikut dan sesuaikan path file:\n",
    "# df = pd.read_csv('data_iklim.csv')\n",
    "df['Tanggal'] = pd.to_datetime(df['Tanggal'])\n",
    "df = df.sort_values('Tanggal').reset_index(drop=True)\n",
    "\n",
    "print(\"Data berhasil dimuat!\")\n",
    "# print(f\"Jumlah data: {len(df)}\")\n",
    "# print(f\"\\nInfo data:\\n{df.info()}\")\n",
    "# print(f\"\\nSample data:\\n{df.head()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PREPROCESSING DATA\n",
    "# ============================================================================\n",
    "\n",
    "def interpolate_missing_data(df):\n",
    "\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Interpolasi untuk kolom numerik\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df_clean[col] = df_clean[col].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def extract_weekly_features(df):\n",
    "    df_weekly = df.copy()\n",
    "    df_weekly['Week'] = df_weekly['Tanggal'].dt.isocalendar().week\n",
    "    df_weekly['Year'] = df_weekly['Tanggal'].dt.year\n",
    "    \n",
    "    # Agregasi mingguan - ambil nilai maksimum (HANYA 3 KOLOM)\n",
    "    weekly_data = df_weekly.groupby(['Year', 'Week']).agg({\n",
    "        'Temperatur Rata-rata': 'max',\n",
    "        'Kelembapan Rata-rata': 'max',\n",
    "        'Curah Hujan (mm)': 'max',\n",
    "        'Tanggal': 'last'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return weekly_data\n",
    "\n",
    "def normalize_data(df, feature_cols):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "    return df_normalized, scaler\n",
    "\n",
    "def classify_rainfall(rainfall):\n",
    "    if rainfall < 5:\n",
    "        return 0\n",
    "    elif rainfall < 20:\n",
    "        return 1\n",
    "    elif rainfall < 50:\n",
    "        return 2\n",
    "    elif rainfall < 100:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Uncomment untuk menjalankan preprocessing:\n",
    "\n",
    "# 3.1 Interpolasi missing data\n",
    "df = interpolate_missing_data(df)\n",
    "print(\"Missing data berhasil diinterpolasi!\")\n",
    "\n",
    "# 3.2 Ekstraksi fitur mingguan\n",
    "df_weekly = extract_weekly_features(df)\n",
    "print(f\"Data mingguan berhasil diekstrak! Total: {len(df_weekly)} minggu\")\n",
    "\n",
    "# 3.3 Klasifikasi curah hujan\n",
    "df_weekly['Rainfall_Class'] = df_weekly['Curah Hujan (mm)'].apply(classify_rainfall)\n",
    "\n",
    "# 3.4 Normalisasi - HANYA 3 KOLOM SESUAI PAPER\n",
    "feature_cols = ['Temperatur Rata-rata', 'Kelembapan Rata-rata', 'Curah Hujan (mm)']\n",
    "\n",
    "df_normalized, scaler = normalize_data(df_weekly, feature_cols)\n",
    "print(\"Data berhasil dinormalisasi!\")\n",
    "print(f\"Jumlah fitur yang digunakan: {len(feature_cols)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PREPARE SEQUENCES (OVERLAPPING WINDOWS)\n",
    "# ============================================================================\n",
    "\n",
    "def create_sequences(data, feature_cols, target_col, sequence_length=52):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # Ambil sequence 52 minggu\n",
    "        sequence = data[feature_cols].iloc[i:i+sequence_length].values\n",
    "        # Target adalah minggu ke-53 (prediksi minggu depan)\n",
    "        target = data[target_col].iloc[i+sequence_length]\n",
    "        \n",
    "        X.append(sequence)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Uncomment untuk membuat sequences:\n",
    "\n",
    "sequence_length = 52  # 1 tahun = 52 minggu\n",
    "\n",
    "X, y = create_sequences(df_normalized, feature_cols, 'Rainfall_Class', sequence_length)\n",
    "\n",
    "print(f\"Shape X: {X.shape}\")  # (samples, 52, n_features)\n",
    "print(f\"Shape y: {y.shape}\")  # (samples,)\n",
    "print(f\"Total sequences: {len(X)}\")\n",
    "\n",
    "# Visualisasi distribusi kelas\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=5, edgecolor='black')\n",
    "plt.xlabel('Rainfall Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Rainfall Classes')\n",
    "plt.xticks([0, 1, 2, 3, 4], ['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "class_names = ['Very Low\\n(<5mm)', 'Low\\n(5-20mm)', 'Medium\\n(20-50mm)', \n",
    "               'High\\n(50-100mm)', 'Very High\\n(>100mm)']\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "plt.bar(unique, counts, edgecolor='black')\n",
    "plt.xlabel('Rainfall Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rainfall Class Distribution')\n",
    "plt.xticks(unique, class_names, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SPLIT DATA (80% TRAINING, 20% TESTING)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk split data:\n",
    "\n",
    "# Split data sesuai paper: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Convert target ke categorical (one-hot encoding)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=5)\n",
    "y_test_cat = to_categorical(y_test, num_classes=5)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. BUILD RNN-LSTM MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def build_lstm_model(input_shape, num_classes=5, lstm_units=64, dropout_rate=0.2):\n",
    "    model = Sequential([\n",
    "        # Input + LSTM Layer (64 units)\n",
    "        LSTM(lstm_units, input_shape=input_shape, return_sequences=False),\n",
    "        \n",
    "        # Dropout untuk mencegah overfitting\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense Layer (13 neurons dengan ReLU activation)\n",
    "        Dense(13, activation='relu'),\n",
    "        \n",
    "        # Output Layer (5 classes dengan Sigmoid activation)\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment untuk build model:\n",
    "\n",
    "# Input shape: (sequence_length, n_features)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "model = build_lstm_model(input_shape, num_classes=5, lstm_units=64, dropout_rate=0.2)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. COMPILE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk compile model:\n",
    "\n",
    "# Sesuai paper: menggunakan Adam optimizer dengan learning rate 0.001\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model berhasil dikompilasi!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. TRAINING MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk training:\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_rainfall_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training dengan 500 epochs sesuai paper\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training selesai!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. VISUALISASI HASIL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(history):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment untuk visualisasi:\n",
    "plot_training_history(history)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 10. EVALUASI MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk evaluasi:\n",
    "\n",
    "# Evaluasi pada training data\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluasi pada test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Prediksi\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_names = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 11. COMPARISON: RNN vs CNN (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "def build_cnn_model(input_shape, num_classes=5):\n",
    "    from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment untuk perbandingan RNN vs CNN:\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERBANDINGAN RNN vs CNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = build_cnn_model(input_shape, num_classes=5)\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train CNN\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_train_loss, cnn_train_acc = cnn_model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# Comparison Table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['RNN (LSTM)', 'CNN (1D)'],\n",
    "    'Training Accuracy (%)': [train_accuracy*100, cnn_train_acc*100],\n",
    "    'Training Loss': [train_loss, cnn_train_loss],\n",
    "    'Test Accuracy (%)': [test_accuracy*100, cnn_test_acc*100],\n",
    "    'Test Loss': [test_loss, cnn_test_loss]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# 12. COMPARISON: SGD vs ADAM OPTIMIZER (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk perbandingan SGD vs Adam:\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERBANDINGAN SGD vs ADAM OPTIMIZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model dengan SGD\n",
    "model_sgd = build_lstm_model(input_shape, num_classes=5)\n",
    "model_sgd.compile(\n",
    "    optimizer=SGD(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train dengan SGD\n",
    "history_sgd = model_sgd.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate SGD\n",
    "sgd_train_loss, sgd_train_acc = model_sgd.evaluate(X_train, y_train_cat, verbose=0)\n",
    "sgd_test_loss, sgd_test_acc = model_sgd.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# Comparison Table\n",
    "optimizer_comparison = pd.DataFrame({\n",
    "    'Optimizer': ['Adam', 'SGD'],\n",
    "    'Training Accuracy (%)': [train_accuracy*100, sgd_train_acc*100],\n",
    "    'Training Loss': [train_loss, sgd_train_loss],\n",
    "    'Test Accuracy (%)': [test_accuracy*100, sgd_test_acc*100],\n",
    "    'Test Loss': [test_loss, sgd_test_loss]\n",
    "})\n",
    "\n",
    "print(optimizer_comparison.to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 13. TESTING DIFFERENT LEARNING RATES (OPSIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk testing learning rates:\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING DIFFERENT LEARNING RATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "learning_rates = [0.001, 0.002, 0.010, 0.040, 0.100, 0.400, 0.600, 0.800]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    \n",
    "    # Build and compile model\n",
    "    model_lr = build_lstm_model(input_shape, num_classes=5)\n",
    "    model_lr.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history_lr = model_lr.fit(\n",
    "        X_train, y_train_cat,\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test_cat),\n",
    "        callbacks=[EarlyStopping(patience=30, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss_lr, train_acc_lr = model_lr.evaluate(X_train, y_train_cat, verbose=0)\n",
    "    test_loss_lr, test_acc_lr = model_lr.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    \n",
    "    lr_results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Training Accuracy (%)': train_acc_lr*100,\n",
    "        'Training Loss': train_loss_lr,\n",
    "        'Test Accuracy (%)': test_acc_lr*100,\n",
    "        'Test Loss': test_loss_lr\n",
    "    })\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"\\n\" + lr_df.to_string(index=False))\n",
    "\n",
    "# Visualize learning rate comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(lr_df['Learning Rate'], lr_df['Training Accuracy (%)'], \n",
    "             marker='o', label='Training', linewidth=2)\n",
    "axes[0].plot(lr_df['Learning Rate'], lr_df['Test Accuracy (%)'], \n",
    "             marker='s', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Accuracy vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "axes[1].plot(lr_df['Learning Rate'], lr_df['Training Loss'], \n",
    "             marker='o', label='Training', linewidth=2)\n",
    "axes[1].plot(lr_df['Learning Rate'], lr_df['Test Loss'], \n",
    "             marker='s', label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Loss vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 14. PREDICTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def predict_rainfall(model, input_sequence, scaler):\n",
    "    # Reshape untuk prediksi\n",
    "    input_seq = input_sequence.reshape(1, input_sequence.shape[0], input_sequence.shape[1])\n",
    "    \n",
    "    # Prediksi\n",
    "    prediction_probs = model.predict(input_seq, verbose=0)\n",
    "    predicted_class = np.argmax(prediction_probs, axis=1)[0]\n",
    "    confidence = prediction_probs[0][predicted_class] * 100\n",
    "    \n",
    "    # Mapping kelas ke nama\n",
    "    class_mapping = {\n",
    "        0: 'Very Low (<5mm)',\n",
    "        1: 'Low (5-20mm)',\n",
    "        2: 'Medium (20-50mm)',\n",
    "        3: 'High (50-100mm)',\n",
    "        4: 'Very High (>100mm)'\n",
    "    }\n",
    "    \n",
    "    class_name = class_mapping[predicted_class]\n",
    "    \n",
    "    return predicted_class, class_name, confidence\n",
    "\n",
    "# Uncomment untuk testing prediksi:\n",
    "# Contoh prediksi\n",
    "sample_sequence = X_test[0]\n",
    "pred_class, pred_name, pred_confidence = predict_rainfall(model, sample_sequence, scaler)\n",
    "\n",
    "print(f\"\\nPrediksi Curah Hujan Minggu Depan:\")\n",
    "print(f\"Kelas: {pred_name}\")\n",
    "print(f\"Confidence: {pred_confidence:.2f}%\")\n",
    "print(f\"Actual: {class_mapping[y_test[0]]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 15. SAVE AND LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment untuk save model:\n",
    "# Save model\n",
    "model.save('rainfall_forecast_model.h5')\n",
    "print(\"Model berhasil disimpan!\")\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Scaler berhasil disimpan!\")\n",
    "\n",
    "# Uncomment untuk load model:\n",
    "\n",
    "# Load model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('rainfall_forecast_model.h5')\n",
    "\n",
    "# Load scaler\n",
    "loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "print(\"Model dan scaler berhasil dimuat!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCRIPT SELESAI!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCatatan:\")\n",
    "print(\"- Uncomment bagian-bagian kode sesuai kebutuhan\")\n",
    "print(\"- Pastikan data sudah dimuat dengan benar sebelum menjalankan\")\n",
    "print(\"- Sesuaikan path file untuk load dan save data/model\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a667122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    roc_auc_score, classification_report\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70723b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_excel(\"updated_dataset_final.xlsx\")\n",
    "\n",
    "# Pastikan format tanggal\n",
    "df[\"Tanggal\"] = pd.to_datetime(df[\"Tanggal\"])\n",
    "\n",
    "# Sort time (WAJIB)\n",
    "df = df.sort_values(\"Tanggal\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "815f7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING_CODE = 8888\n",
    "df.replace(MISSING_CODE, np.nan, inplace=True)\n",
    "\n",
    "df[\"RainTomorrow\"] = (df[\"Curah Hujan (mm)\"].shift(-1) >= 1.0).astype(int)\n",
    "df = df.iloc[:-1]  # drop baris terakhir (target NaN)\n",
    "\n",
    "df = df.drop(columns=[\"Curah Hujan (mm)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4cc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Rain_lag1\"] = df[\"RainTomorrow\"].shift(1)\n",
    "df[\"Rain_lag3\"] = df[\"RainTomorrow\"].shift(3)\n",
    "\n",
    "df[\"wind_sin\"] = np.sin(np.deg2rad(df[\"Arah Angin Terbanyak (Â°)\"]))\n",
    "df[\"wind_cos\"] = np.cos(np.deg2rad(df[\"Arah Angin Terbanyak (Â°)\"]))\n",
    "\n",
    "df = df.drop(columns=[\"Arah Angin Terbanyak (Â°)\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df.columns.drop([\"Tanggal\", \"RainTomorrow\"])\n",
    "\n",
    "for col in feature_cols:\n",
    "    df[col + \"_missing\"] = df[col].isna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b18453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6625 - loss: 0.6072 - val_accuracy: 0.7030 - val_loss: 0.5945\n",
      "Epoch 2/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6739 - loss: 0.5818 - val_accuracy: 0.7047 - val_loss: 0.5923\n",
      "Epoch 3/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6764 - loss: 0.5789 - val_accuracy: 0.7097 - val_loss: 0.5928\n",
      "Epoch 4/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6825 - loss: 0.5716 - val_accuracy: 0.6997 - val_loss: 0.5915\n",
      "Epoch 5/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6842 - loss: 0.5715 - val_accuracy: 0.6930 - val_loss: 0.5923\n",
      "Epoch 6/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6907 - loss: 0.5693 - val_accuracy: 0.6980 - val_loss: 0.5912\n",
      "Epoch 7/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6889 - loss: 0.5635 - val_accuracy: 0.6930 - val_loss: 0.5904\n",
      "Epoch 8/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6939 - loss: 0.5589 - val_accuracy: 0.6896 - val_loss: 0.5912\n",
      "Epoch 9/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6928 - loss: 0.5613 - val_accuracy: 0.6896 - val_loss: 0.5922\n",
      "Epoch 10/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6981 - loss: 0.5571 - val_accuracy: 0.6896 - val_loss: 0.5924\n",
      "Epoch 11/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6989 - loss: 0.5488 - val_accuracy: 0.6896 - val_loss: 0.5911\n",
      "Epoch 12/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6992 - loss: 0.5481 - val_accuracy: 0.6829 - val_loss: 0.5923\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\n",
      "===== TEST PERFORMANCE =====\n",
      "Accuracy : 0.639261744966443\n",
      "Precision: 0.55\n",
      "Recall   : 0.38095238095238093\n",
      "ROC AUC  : 0.6547470794046136\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73       365\n",
      "           1       0.55      0.38      0.45       231\n",
      "\n",
      "    accuracy                           0.64       596\n",
      "   macro avg       0.61      0.59      0.59       596\n",
      "weighted avg       0.62      0.64      0.62       596\n",
      "\n",
      "\n",
      "===== LAPORAN CUACA BESOK =====\n",
      "Prediksi Hujan : TIDAK\n",
      "Probabilitas   : 45.26%\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0. IMPORT LIBRARY\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    roc_auc_score, classification_report\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD & SORT DATA\n",
    "# =========================\n",
    "df = pd.read_excel(\"updated_dataset_final.xlsx\")\n",
    "\n",
    "df[\"Tanggal\"] = pd.to_datetime(df[\"Tanggal\"])\n",
    "df = df.sort_values(\"Tanggal\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. HANDLE 8888 â NaN\n",
    "# =========================\n",
    "MISSING_CODE = 8888\n",
    "df.replace(MISSING_CODE, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. DEFINE TARGET (RAIN TOMORROW)\n",
    "# =========================\n",
    "df[\"RainTomorrow\"] = (df[\"Curah Hujan (mm)\"].shift(-1) >= 1.0).astype(int)\n",
    "df = df.iloc[:-1]   # drop baris terakhir\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. DROP LEAKAGE FEATURE\n",
    "# =========================\n",
    "df = df.drop(columns=[\"Curah Hujan (mm)\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. FEATURE ENGINEERING\n",
    "# =========================\n",
    "\n",
    "# Lag hujan (masa lalu saja)\n",
    "df[\"Rain_lag1\"] = df[\"RainTomorrow\"].shift(1)\n",
    "df[\"Rain_lag3\"] = df[\"RainTomorrow\"].shift(3)\n",
    "\n",
    "# Arah angin â circular encoding\n",
    "df[\"wind_sin\"] = np.sin(np.deg2rad(df[\"Arah Angin Terbanyak (Â°)\"]))\n",
    "df[\"wind_cos\"] = np.cos(np.deg2rad(df[\"Arah Angin Terbanyak (Â°)\"]))\n",
    "\n",
    "df = df.drop(columns=[\"Arah Angin Terbanyak (Â°)\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. MISSING INDICATOR\n",
    "# =========================\n",
    "feature_cols = df.columns.drop([\"Tanggal\", \"RainTomorrow\"])\n",
    "\n",
    "for col in feature_cols:\n",
    "    df[col + \"_missing\"] = df[col].isna().astype(int)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7. TIME-BASED SPLIT\n",
    "# =========================\n",
    "n = len(df)\n",
    "\n",
    "train_end = int(0.7 * n)\n",
    "val_end   = int(0.85 * n)\n",
    "\n",
    "train = df.iloc[:train_end]\n",
    "val   = df.iloc[train_end:val_end]\n",
    "test  = df.iloc[val_end:]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8. IMPUTATION (NO LEAKAGE)\n",
    "# =========================\n",
    "train = train.ffill()\n",
    "val   = val.ffill()\n",
    "test  = test.ffill()\n",
    "\n",
    "median_vals = train.median(numeric_only=True)\n",
    "\n",
    "train = train.fillna(median_vals)\n",
    "val   = val.fillna(median_vals)\n",
    "test  = test.fillna(median_vals)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9. PREPARE X & y\n",
    "# =========================\n",
    "X_train = train.drop(columns=[\"Tanggal\", \"RainTomorrow\"])\n",
    "y_train = train[\"RainTomorrow\"]\n",
    "\n",
    "X_val = val.drop(columns=[\"Tanggal\", \"RainTomorrow\"])\n",
    "y_val = val[\"RainTomorrow\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"Tanggal\", \"RainTomorrow\"])\n",
    "y_test = test[\"RainTomorrow\"]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10. SCALING (TRAIN ONLY)\n",
    "# =========================\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 11. WINDOWING FOR RNN\n",
    "# =========================\n",
    "def make_sequences(X, y, window=7):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        X_seq.append(X[i-window:i])\n",
    "        y_seq.append(y.iloc[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "WINDOW = 7\n",
    "\n",
    "X_train_seq, y_train_seq = make_sequences(X_train, y_train, WINDOW)\n",
    "X_val_seq, y_val_seq     = make_sequences(X_val, y_val, WINDOW)\n",
    "X_test_seq, y_test_seq   = make_sequences(X_test, y_test, WINDOW)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 12. BUILD RNN MODEL\n",
    "# =========================\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 13. TRAIN MODEL\n",
    "# =========================\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 14. EVALUATION (TEST SET)\n",
    "# =========================\n",
    "y_prob = model.predict(X_test_seq).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n===== TEST PERFORMANCE =====\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test_seq, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test_seq, y_pred))\n",
    "print(\"Recall   :\", recall_score(y_test_seq, y_pred))\n",
    "print(\"ROC AUC  :\", roc_auc_score(y_test_seq, y_prob))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test_seq, y_pred))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 15. DAILY WEATHER REPORT\n",
    "# =========================\n",
    "latest_prob = y_prob[-1] * 100\n",
    "\n",
    "print(\"\\n===== LAPORAN CUACA BESOK =====\")\n",
    "print(\"Prediksi Hujan :\", \"YA\" if latest_prob >= 50 else \"TIDAK\")\n",
    "print(f\"Probabilitas   : {latest_prob:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d5b60b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[293  72]\n",
      " [143  88]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_seq, y_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ff9859",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '-'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2556\\2090785250.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    128\u001b[39m X_test = test.drop(columns=[\u001b[33m\"Tanggal\"\u001b[39m, \u001b[33m\"RainTomorrow\"\u001b[39m])\n\u001b[32m    129\u001b[39m y_test = test[\u001b[33m\"RainTomorrow\"\u001b[39m]\n\u001b[32m    130\u001b[39m \n\u001b[32m    131\u001b[39m scaler = StandardScaler()\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m X_train = scaler.fit_transform(X_train)\n\u001b[32m    133\u001b[39m X_val   = scaler.transform(X_val)\n\u001b[32m    134\u001b[39m X_test  = scaler.transform(X_test)\n\u001b[32m    135\u001b[39m \n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    890\u001b[39m                 )\n\u001b[32m    891\u001b[39m \n\u001b[32m    892\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    903\u001b[39m             Fitted scaler.\n\u001b[32m    904\u001b[39m         \"\"\"\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m         self._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.partial_fit(X, y, sample_weight)\n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1361\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m                 )\n\u001b[32m   1364\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    939\u001b[39m         self : object\n\u001b[32m    940\u001b[39m             Fitted scaler.\n\u001b[32m    941\u001b[39m         \"\"\"\n\u001b[32m    942\u001b[39m         first_call = \u001b[38;5;28;01mnot\u001b[39;00m hasattr(self, \u001b[33m\"n_samples_seen_\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m         X = validate_data(\n\u001b[32m    944\u001b[39m             self,\n\u001b[32m    945\u001b[39m             X,\n\u001b[32m    946\u001b[39m             accept_sparse=(\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m),\n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2950\u001b[39m             out = y\n\u001b[32m   2951\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2952\u001b[39m             out = X, y\n\u001b[32m   2953\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2955\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2957\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1050\u001b[39m                         )\n\u001b[32m   1051\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1053\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m                 raise ValueError(\n\u001b[32m   1056\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    754\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    755\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    758\u001b[39m \n\u001b[32m    759\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Lenovo E15\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2164\u001b[39m             )\n\u001b[32m   2165\u001b[39m         values = self._values\n\u001b[32m   2166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2167\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2171\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '-'"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "#               WEATHER RAIN PREDICTION â FULL SCRIPT\n",
    "# =====================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 1. LOAD DATA\n",
    "# ===========================================================\n",
    "df = pd.read_excel(\"Updated_Data_Historis_2015_2025.xlsx\")\n",
    "df[\"Tanggal\"] = pd.to_datetime(df[\"Tanggal\"])\n",
    "df = df.sort_values(\"Tanggal\").reset_index(drop=True)\n",
    "\n",
    "# ===========================================================\n",
    "# 2. HANDLE MISSING 8888 â NaN\n",
    "# ===========================================================\n",
    "df.replace(8888, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# FIX: pastikan Curah Hujan (mm) numerik\n",
    "df[\"Curah Hujan (mm)\"] = (\n",
    "    df[\"Curah Hujan (mm)\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df[\"Curah Hujan (mm)\"] = pd.to_numeric(df[\"Curah Hujan (mm)\"], errors=\"coerce\")\n",
    "\n",
    "# TARGET: hujan besok\n",
    "df[\"RainTomorrow\"] = (df[\"Curah Hujan (mm)\"].shift(-1) >= 1.0).astype(int)\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "# ===========================================================\n",
    "# 3. TARGET: HUJAN BESOK\n",
    "# ===========================================================\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 4. LAG CURAH HUJAN (PALING PENTING)\n",
    "# ===========================================================\n",
    "df[\"Rain_mm_lag1\"] = df[\"Curah Hujan (mm)\"].shift(1)\n",
    "df[\"Rain_mm_lag3\"] = df[\"Curah Hujan (mm)\"].shift(3)\n",
    "df[\"Rain_mm_lag7\"] = df[\"Curah Hujan (mm)\"].shift(7)\n",
    "df.drop(columns=[\"Curah Hujan (mm)\"], inplace=True)\n",
    "\n",
    "# ===========================================================\n",
    "# 5. MUSIM (SIN-COS)\n",
    "# ===========================================================\n",
    "df[\"month\"] = df[\"Tanggal\"].dt.month\n",
    "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "df.drop(columns=[\"month\"], inplace=True)\n",
    "\n",
    "# ===========================================================\n",
    "# 6. CIRCULAR WIND DIRECTION\n",
    "# ===========================================================\n",
    "\n",
    "df[\"Arah Angin Terbanyak (Â°)\"] = (\n",
    "    df[\"Arah Angin Terbanyak (Â°)\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df[\"Arah Angin Terbanyak (Â°)\"] = pd.to_numeric(\n",
    "    df[\"Arah Angin Terbanyak (Â°)\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "\n",
    "df[\"wind_sin\"] = np.sin(np.deg2rad(df[\"Arah Angin Terbanyak (Â°)\"]))\n",
    "df[\"wind_cos\"] = np.cos(np.deg2rad(df[\"Arah Angin Terbanyak (Â°)\"]))\n",
    "df.drop(columns=[\"Arah Angin Terbanyak (Â°)\"], inplace=True)\n",
    "\n",
    "# ===========================================================\n",
    "# 7. MISSING INDICATOR\n",
    "# ===========================================================\n",
    "feature_cols = df.columns.drop([\"Tanggal\", \"RainTomorrow\"])\n",
    "for col in feature_cols:\n",
    "    df[col + \"_missing\"] = df[col].isna().astype(int)\n",
    "\n",
    "# ===========================================================\n",
    "# 8. TIME SERIES SPLIT\n",
    "# ===========================================================\n",
    "n = len(df)\n",
    "train_end = int(0.7 * n)\n",
    "val_end   = int(0.85 * n)\n",
    "\n",
    "train = df.iloc[:train_end]\n",
    "val   = df.iloc[train_end:val_end]\n",
    "test  = df.iloc[val_end:]\n",
    "\n",
    "# ===========================================================\n",
    "# 9. IMPUTATION (ANTI LEAKAGE)\n",
    "# ===========================================================\n",
    "train = train.ffill()\n",
    "val   = val.ffill()\n",
    "test  = test.ffill()\n",
    "\n",
    "median_vals = train.median(numeric_only=True)\n",
    "train = train.fillna(median_vals)\n",
    "val   = val.fillna(median_vals)\n",
    "test  = test.fillna(median_vals)\n",
    "\n",
    "# ===========================================================\n",
    "# 10. SCALING\n",
    "# ===========================================================\n",
    "X_train = train.drop(columns=[\"Tanggal\", \"RainTomorrow\"])\n",
    "y_train = train[\"RainTomorrow\"]\n",
    "\n",
    "X_val = val.drop(columns=[\"Tanggal\", \"RainTomorrow\"])\n",
    "y_val = val[\"RainTomorrow\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"Tanggal\", \"RainTomorrow\"])\n",
    "y_test = test[\"RainTomorrow\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# ===========================================================\n",
    "# 11. WINDOWING\n",
    "# ===========================================================\n",
    "def make_sequences(X, y, window=30):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        X_seq.append(X[i-window:i])\n",
    "        y_seq.append(y.iloc[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "WINDOW = 30\n",
    "\n",
    "X_train_seq, y_train_seq = make_sequences(X_train, y_train)\n",
    "X_val_seq, y_val_seq     = make_sequences(X_val, y_val)\n",
    "X_test_seq, y_test_seq   = make_sequences(X_test, y_test)\n",
    "\n",
    "# ===========================================================\n",
    "# 12. CLASS WEIGHTS\n",
    "# ===========================================================\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0,1]),\n",
    "    y=y_train_seq\n",
    ")\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "# ===========================================================\n",
    "# 13. MODEL LSTM\n",
    "# ===========================================================\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True,\n",
    "         input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[tf.keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 14. TRAIN\n",
    "# ===========================================================\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_auc\",\n",
    "    patience=7,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 15. EVALUATION\n",
    "# ===========================================================\n",
    "y_prob = model.predict(X_test_seq).ravel()\n",
    "threshold = 0.35   # lebih sensitif terhadap hujan\n",
    "y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "print(\"\\n===== TEST RESULTS =====\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test_seq, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test_seq, y_pred))\n",
    "print(\"Recall   :\", recall_score(y_test_seq, y_pred))\n",
    "print(\"ROC AUC  :\", roc_auc_score(y_test_seq, y_prob))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_seq, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_seq, y_pred))\n",
    "\n",
    "# ===========================================================\n",
    "# 16. WEATHER FORECAST FOR NEXT DAY\n",
    "# ===========================================================\n",
    "prob = y_prob[-1] * 100\n",
    "print(\"\\n===== LAPORAN CUACA BESOK =====\")\n",
    "print(\"Prediksi Hujan :\", \"YA\" if prob >= 50 else \"TIDAK\")\n",
    "print(f\"Probabilitas   : {prob:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec83ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4019 entries, 0 to 4018\n",
      "Data columns (total 32 columns):\n",
      " #   Column                                      Non-Null Count  Dtype         \n",
      "---  ------                                      --------------  -----         \n",
      " 0   Tanggal                                     4019 non-null   datetime64[ns]\n",
      " 1   Temperatur Minimum                          4019 non-null   object        \n",
      " 2   Temperatur Maksimum                         4019 non-null   object        \n",
      " 3   Temperatur Rata-rata                        4019 non-null   object        \n",
      " 4   Kelembapan Rata-rata                        4019 non-null   object        \n",
      " 5   Lamanya Penyinaran Matahari                 4019 non-null   object        \n",
      " 6   Kecepatan Angin Maksimum                    4019 non-null   int64         \n",
      " 7   Arah Angin Saat Kecepatan Maksimum          4019 non-null   int64         \n",
      " 8   Kecepatan Angin Rata-rata                   4019 non-null   int64         \n",
      " 9   RainTomorrow                                4019 non-null   int64         \n",
      " 10  Rain_mm_lag1                                3312 non-null   float64       \n",
      " 11  Rain_mm_lag3                                3310 non-null   float64       \n",
      " 12  Rain_mm_lag7                                3307 non-null   float64       \n",
      " 13  month_sin                                   4019 non-null   float64       \n",
      " 14  month_cos                                   4019 non-null   float64       \n",
      " 15  wind_sin                                    0 non-null      float64       \n",
      " 16  wind_cos                                    0 non-null      float64       \n",
      " 17  Temperatur Minimum_missing                  4019 non-null   int64         \n",
      " 18  Temperatur Maksimum_missing                 4019 non-null   int64         \n",
      " 19  Temperatur Rata-rata_missing                4019 non-null   int64         \n",
      " 20  Kelembapan Rata-rata_missing                4019 non-null   int64         \n",
      " 21  Lamanya Penyinaran Matahari_missing         4019 non-null   int64         \n",
      " 22  Kecepatan Angin Maksimum_missing            4019 non-null   int64         \n",
      " 23  Arah Angin Saat Kecepatan Maksimum_missing  4019 non-null   int64         \n",
      " 24  Kecepatan Angin Rata-rata_missing           4019 non-null   int64         \n",
      " 25  Rain_mm_lag1_missing                        4019 non-null   int64         \n",
      " 26  Rain_mm_lag3_missing                        4019 non-null   int64         \n",
      " 27  Rain_mm_lag7_missing                        4019 non-null   int64         \n",
      " 28  month_sin_missing                           4019 non-null   int64         \n",
      " 29  month_cos_missing                           4019 non-null   int64         \n",
      " 30  wind_sin_missing                            4019 non-null   int64         \n",
      " 31  wind_cos_missing                            4019 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(7), int64(19), object(5)\n",
      "memory usage: 1004.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0924ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
